{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954bdf9a",
   "metadata": {},
   "source": [
    "# Actividad: Lasso vs. Ridge - Comparación de Modelos de Regularización\n",
    "\n",
    "## Objetivo\n",
    "Comparar empíricamente el rendimiento y la interpretabilidad de las regresiones Lasso y Ridge usando datos simulados que incluyen variables relevantes e irrelevantes para demostrar la capacidad de Lasso de realizar selección automática de características.\n",
    "\n",
    "## Hipótesis\n",
    "1. Lasso tendrá un error de predicción similar o mejor que Ridge\n",
    "2. Lasso producirá un modelo más interpretable al reducir a cero los coeficientes de variables irrelevantes\n",
    "3. Ridge mantendrá todos los coeficientes pero con valores pequeños"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db434da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de gráficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75421982",
   "metadata": {},
   "source": [
    "## Paso 1: Generación de Datos Simulados\n",
    "\n",
    "Crearemos un conjunto de datos que incluya:\n",
    "- Variables relevantes que realmente afectan el ingreso\n",
    "- Variables irrelevantes que no tienen relación con el ingreso\n",
    "- Ruido para simular condiciones reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a5a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parámetros de la simulación\n",
    "n_samples = 1000\n",
    "n_relevant_features = 10  # Variables que realmente afectan el ingreso\n",
    "n_irrelevant_features = 20  # Variables que no afectan el ingreso\n",
    "n_total_features = n_relevant_features + n_irrelevant_features\n",
    "\n",
    "# Generar variables explicativas\n",
    "X = np.random.randn(n_samples, n_total_features)\n",
    "\n",
    "# Definir coeficientes reales (solo las primeras 10 variables son relevantes)\n",
    "true_coefficients = np.zeros(n_total_features)\n",
    "true_coefficients[:n_relevant_features] = np.array([\n",
    "    5000,  # Educación\n",
    "    3000,  # Experiencia laboral\n",
    "    2000,  # Edad\n",
    "    1500,  # Horas trabajadas\n",
    "    1000,  # Sector económico\n",
    "    800,   # Tamaño de empresa\n",
    "    600,   # Nivel de responsabilidad\n",
    "    400,   # Ubicación geográfica\n",
    "    300,   # Certificaciones\n",
    "    200    # Idiomas\n",
    "])\n",
    "\n",
    "# Generar variable objetivo con ruido\n",
    "y = X @ true_coefficients + np.random.normal(0, 1000, n_samples)\n",
    "\n",
    "# Crear DataFrame con nombres de variables\n",
    "feature_names = []\n",
    "for i in range(n_relevant_features):\n",
    "    feature_names.append(f'Variable_Relevante_{i+1}')\n",
    "for i in range(n_irrelevant_features):\n",
    "    feature_names.append(f'Variable_Irrelevante_{i+1}')\n",
    "\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['ingreso_anual'] = y\n",
    "\n",
    "print(f\"Dataset creado con {n_samples} observaciones y {n_total_features} variables\")\n",
    "print(f\"Variables relevantes: {n_relevant_features}\")\n",
    "print(f\"Variables irrelevantes: {n_irrelevant_features}\")\n",
    "print(f\"Rango de ingresos: ${y.min():.0f} - ${y.max():.0f}\")\n",
    "print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bd926e",
   "metadata": {},
   "source": [
    "## Paso 2: Preparación de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar variables explicativas y objetivo\n",
    "X = df.drop('ingreso_anual', axis=1)\n",
    "y = df['ingreso_anual']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Estandarizar las variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Conjunto de entrenamiento: {X_train.shape[0]} observaciones\")\n",
    "print(f\"Conjunto de prueba: {X_test.shape[0]} observaciones\")\n",
    "print(f\"Número de variables: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de64c4",
   "metadata": {},
   "source": [
    "## Paso 3: Entrenamiento del Modelo Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir valores de alpha para Ridge\n",
    "alpha_values = np.logspace(-3, 3, 50)\n",
    "\n",
    "# Entrenar Ridge con validación cruzada\n",
    "ridge_scores = []\n",
    "for alpha in alpha_values:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    scores = cross_val_score(ridge, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    ridge_scores.append(-np.mean(scores))\n",
    "\n",
    "# Encontrar el mejor alpha\n",
    "best_alpha_ridge = alpha_values[np.argmin(ridge_scores)]\n",
    "\n",
    "# Entrenar modelo Ridge final con el mejor alpha\n",
    "ridge_model = Ridge(alpha=best_alpha_ridge)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Mejor alpha para Ridge: {best_alpha_ridge:.4f}\")\n",
    "print(f\"RMSE Ridge: ${rmse_ridge:.2f}\")\n",
    "print(f\"R² Ridge: {r2_ridge:.4f}\")\n",
    "\n",
    "# Contar coeficientes no cero\n",
    "ridge_non_zero = np.sum(ridge_model.coef_ != 0)\n",
    "print(f\"Coeficientes no cero en Ridge: {ridge_non_zero}/{len(ridge_model.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8f982",
   "metadata": {},
   "source": [
    "## Paso 4: Entrenamiento del Modelo Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir valores de alpha para Lasso\n",
    "alpha_values_lasso = np.logspace(-3, 1, 50)\n",
    "\n",
    "# Entrenar Lasso con validación cruzada\n",
    "lasso_scores = []\n",
    "for alpha in alpha_values_lasso:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=2000)\n",
    "    scores = cross_val_score(lasso, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    lasso_scores.append(-np.mean(scores))\n",
    "\n",
    "# Encontrar el mejor alpha\n",
    "best_alpha_lasso = alpha_values_lasso[np.argmin(lasso_scores)]\n",
    "\n",
    "# Entrenar modelo Lasso final con el mejor alpha\n",
    "lasso_model = Lasso(alpha=best_alpha_lasso, max_iter=2000)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_lasso = lasso_model.predict(X_test_scaled)\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f\"Mejor alpha para Lasso: {best_alpha_lasso:.4f}\")\n",
    "print(f\"RMSE Lasso: ${rmse_lasso:.2f}\")\n",
    "print(f\"R² Lasso: {r2_lasso:.4f}\")\n",
    "\n",
    "# Contar coeficientes no cero\n",
    "lasso_non_zero = np.sum(lasso_model.coef_ != 0)\n",
    "print(f\"Coeficientes no cero en Lasso: {lasso_non_zero}/{len(lasso_model.coef_)}\")\n",
    "print(f\"Variables seleccionadas por Lasso: {lasso_non_zero}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8fc87",
   "metadata": {},
   "source": [
    "## Paso 5: Comparación Visual de Coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear figura con subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Gráfico de coeficientes Ridge\n",
    "coef_ridge = pd.Series(ridge_model.coef_, index=feature_names)\n",
    "coef_ridge_sorted = coef_ridge.sort_values(key=abs, ascending=False)\n",
    "coef_ridge_sorted.plot(kind='bar', ax=ax1, color='skyblue', alpha=0.7)\n",
    "ax1.set_title('Coeficientes del Modelo Ridge', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Valor del Coeficiente')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico de coeficientes Lasso\n",
    "coef_lasso = pd.Series(lasso_model.coef_, index=feature_names)\n",
    "coef_lasso_sorted = coef_lasso.sort_values(key=abs, ascending=False)\n",
    "coef_lasso_sorted.plot(kind='bar', ax=ax2, color='lightcoral', alpha=0.7)\n",
    "ax2.set_title('Coeficientes del Modelo Lasso', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Valor del Coeficiente')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar las variables más importantes según cada modelo\n",
    "print(\"\\n=== TOP 10 VARIABLES MÁS IMPORTANTES ===\")\n",
    "print(\"\\nRidge (por valor absoluto):\")\n",
    "for i, (var, coef) in enumerate(coef_ridge_sorted.head(10).items()):\n",
    "    print(f\"{i+1:2d}. {var}: {coef:.2f}\")\n",
    "\n",
    "print(\"\\nLasso (variables seleccionadas):\")\n",
    "lasso_selected = coef_lasso[coef_lasso != 0].sort_values(key=abs, ascending=False)\n",
    "for i, (var, coef) in enumerate(lasso_selected.items()):\n",
    "    print(f\"{i+1:2d}. {var}: {coef:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774c134",
   "metadata": {},
   "source": [
    "## Paso 6: Tabla de Resultados Comparativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee263df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla de resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'Métrica': ['RMSE (Error de Predicción)', \n",
    "                'R² (Coeficiente de Determinación)',\n",
    "                'Número de Variables Usadas',\n",
    "                'Variables Relevantes Identificadas',\n",
    "                'Variables Irrelevantes Eliminadas'],\n",
    "    'Ridge': [f\"${rmse_ridge:.2f}\", \n",
    "              f\"{r2_ridge:.4f}\",\n",
    "              f\"{ridge_non_zero}/{len(ridge_model.coef_)}\",\n",
    "              f\"{np.sum(ridge_model.coef_[:n_relevant_features] != 0)}/{n_relevant_features}\",\n",
    "              f\"{np.sum(ridge_model.coef_[n_relevant_features:] == 0)}/{n_irrelevant_features}\"],\n",
    "    'Lasso': [f\"${rmse_lasso:.2f}\", \n",
    "              f\"{r2_lasso:.4f}\",\n",
    "              f\"{lasso_non_zero}/{len(lasso_model.coef_)}\",\n",
    "              f\"{np.sum(lasso_model.coef_[:n_relevant_features] != 0)}/{n_relevant_features}\",\n",
    "              f\"{np.sum(lasso_model.coef_[n_relevant_features:] == 0)}/{n_irrelevant_features}\"]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLA DE RESULTADOS: RIDGE vs LASSO\")\n",
    "print(\"=\"*80)\n",
    "print(resultados.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099c232",
   "metadata": {},
   "source": [
    "## Paso 7: Análisis de la Capacidad de Selección de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis detallado de la selección de variables\n",
    "print(\"\\n=== ANÁLISIS DE SELECCIÓN DE VARIABLES ===\")\n",
    "\n",
    "# Variables realmente relevantes (primeras 10)\n",
    "variables_relevantes = feature_names[:n_relevant_features]\n",
    "variables_irrelevantes = feature_names[n_relevant_features:]\n",
    "\n",
    "# Análisis Ridge\n",
    "ridge_relevant_coefs = ridge_model.coef_[:n_relevant_features]\n",
    "ridge_irrelevant_coefs = ridge_model.coef_[n_relevant_features:]\n",
    "\n",
    "print(f\"\\nRIDGE:\")\n",
    "print(f\"- Variables relevantes con coeficiente > 0.1: {np.sum(np.abs(ridge_relevant_coefs) > 0.1)}/{n_relevant_features}\")\n",
    "print(f\"- Variables irrelevantes con coeficiente > 0.1: {np.sum(np.abs(ridge_irrelevant_coefs) > 0.1)}/{n_irrelevant_features}\")\n",
    "print(f\"- Promedio |coef| variables relevantes: {np.mean(np.abs(ridge_relevant_coefs)):.4f}\")\n",
    "print(f\"- Promedio |coef| variables irrelevantes: {np.mean(np.abs(ridge_irrelevant_coefs)):.4f}\")\n",
    "\n",
    "# Análisis Lasso\n",
    "lasso_relevant_coefs = lasso_model.coef_[:n_relevant_features]\n",
    "lasso_irrelevant_coefs = lasso_model.coef_[n_relevant_features:]\n",
    "\n",
    "print(f\"\\nLASSO:\")\n",
    "print(f\"- Variables relevantes seleccionadas: {np.sum(lasso_relevant_coefs != 0)}/{n_relevant_features}\")\n",
    "print(f\"- Variables irrelevantes eliminadas: {np.sum(lasso_irrelevant_coefs == 0)}/{n_irrelevant_features}\")\n",
    "print(f\"- Precisión en selección: {np.sum(lasso_relevant_coefs != 0) / n_relevant_features:.2%}\")\n",
    "print(f\"- Especificidad: {np.sum(lasso_irrelevant_coefs == 0) / n_irrelevant_features:.2%}\")\n",
    "\n",
    "# Mostrar qué variables relevantes fueron identificadas por Lasso\n",
    "print(f\"\\nVariables relevantes identificadas por Lasso:\")\n",
    "for i, (var, coef) in enumerate(zip(variables_relevantes, lasso_relevant_coefs)):\n",
    "    status = \"✓\" if coef != 0 else \"✗\"\n",
    "    print(f\"{status} {var}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b18c1",
   "metadata": {},
   "source": [
    "## Paso 8: Visualización de la Evolución de Coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07430662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar cómo cambian los coeficientes con diferentes valores de alpha\n",
    "alphas_ridge = np.logspace(-3, 3, 20)\n",
    "alphas_lasso = np.logspace(-3, 1, 20)\n",
    "\n",
    "coefs_ridge = []\n",
    "coefs_lasso = []\n",
    "\n",
    "for alpha in alphas_ridge:\n",
    "    ridge_temp = Ridge(alpha=alpha)\n",
    "    ridge_temp.fit(X_train_scaled, y_train)\n",
    "    coefs_ridge.append(ridge_temp.coef_)\n",
    "\n",
    "for alpha in alphas_lasso:\n",
    "    lasso_temp = Lasso(alpha=alpha, max_iter=2000)\n",
    "    lasso_temp.fit(X_train_scaled, y_train)\n",
    "    coefs_lasso.append(lasso_temp.coef_)\n",
    "\n",
    "coefs_ridge = np.array(coefs_ridge)\n",
    "coefs_lasso = np.array(coefs_lasso)\n",
    "\n",
    "# Crear gráficos\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Ridge\n",
    "for i in range(n_total_features):\n",
    "    color = 'red' if i < n_relevant_features else 'gray'\n",
    "    alpha_val = 0.8 if i < n_relevant_features else 0.3\n",
    "    ax1.plot(alphas_ridge, coefs_ridge[:, i], color=color, alpha=alpha_val)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Alpha (Parámetro de Regularización)')\n",
    "ax1.set_ylabel('Coeficientes')\n",
    "ax1.set_title('Evolución de Coeficientes - Ridge')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso\n",
    "for i in range(n_total_features):\n",
    "    color = 'red' if i < n_relevant_features else 'gray'\n",
    "    alpha_val = 0.8 if i < n_relevant_features else 0.3\n",
    "    ax2.plot(alphas_lasso, coefs_lasso[:, i], color=color, alpha=alpha_val)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Alpha (Parámetro de Regularización)')\n",
    "ax2.set_ylabel('Coeficientes')\n",
    "ax2.set_title('Evolución de Coeficientes - Lasso')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservaciones:\")\n",
    "print(\"• Líneas rojas: Variables realmente relevantes\")\n",
    "print(\"• Líneas grises: Variables irrelevantes\")\n",
    "print(\"• Ridge: Los coeficientes se hacen pequeños pero nunca llegan a cero\")\n",
    "print(\"• Lasso: Los coeficientes pueden llegar exactamente a cero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc59c1",
   "metadata": {},
   "source": [
    "## Paso 9: Conclusiones y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5112ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSIONES Y RECOMENDACIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comparar rendimiento predictivo\n",
    "if rmse_lasso < rmse_ridge:\n",
    "    mejor_prediccion = \"Lasso\"\n",
    "    diferencia_rmse = rmse_ridge - rmse_lasso\n",
    "    print(f\"1. RENDIMIENTO PREDICTIVO: Lasso es mejor por ${diferencia_rmse:.2f}\")\n",
    "else:\n",
    "    mejor_prediccion = \"Ridge\"\n",
    "    diferencia_rmse = rmse_lasso - rmse_ridge\n",
    "    print(f\"1. RENDIMIENTO PREDICTIVO: Ridge es mejor por ${diferencia_rmse:.2f}\")\n",
    "\n",
    "# Comparar interpretabilidad\n",
    "reduccion_variables = len(ridge_model.coef_) - lasso_non_zero\n",
    "print(f\"\\n2. INTERPRETABILIDAD:\")\n",
    "print(f\"   • Ridge usa todas las {len(ridge_model.coef_)} variables\")\n",
    "print(f\"   • Lasso usa solo {lasso_non_zero} variables ({reduccion_variables} menos)\")\n",
    "print(f\"   • Lasso eliminó {reduccion_variables/len(ridge_model.coef_)*100:.1f}% de las variables\")\n",
    "\n",
    "# Análisis de selección correcta\n",
    "precision_lasso = np.sum(lasso_relevant_coefs != 0) / n_relevant_features\n",
    "especificidad_lasso = np.sum(lasso_irrelevant_coefs == 0) / n_irrelevant_features\n",
    "\n",
    "print(f\"\\n3. CAPACIDAD DE SELECCIÓN DE VARIABLES:\")\n",
    "print(f\"   • Precisión (variables relevantes identificadas): {precision_lasso:.1%}\")\n",
    "print(f\"   • Especificidad (variables irrelevantes eliminadas): {especificidad_lasso:.1%}\")\n",
    "\n",
    "# Recomendación final\n",
    "print(f\"\\n4. RECOMENDACIÓN FINAL:\")\n",
    "if precision_lasso > 0.7 and especificidad_lasso > 0.8:\n",
    "    print(\"   ✓ Lasso es la mejor opción para este problema\")\n",
    "    print(\"   • Excelente capacidad de selección de variables\")\n",
    "    print(\"   • Modelo más interpretable y simple\")\n",
    "elif rmse_lasso < rmse_ridge * 1.05:  # Si Lasso no es más del 5% peor\n",
    "    print(\"   ✓ Lasso es recomendable\")\n",
    "    print(\"   • Rendimiento predictivo similar a Ridge\")\n",
    "    print(\"   • Ventaja en interpretabilidad\")\n",
    "else:\n",
    "    print(\"   ⚠ Ridge podría ser preferible\")\n",
    "    print(\"   • Mejor rendimiento predictivo\")\n",
    "    print(\"   • Considerar el trade-off con interpretabilidad\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e2d04",
   "metadata": {},
   "source": [
    "## Resumen de la Actividad\n",
    "\n",
    "### Lo que hemos aprendido:\n",
    "\n",
    "1. **Diferencias fundamentales entre Ridge y Lasso:**\n",
    "   - Ridge: Penalización L2, coeficientes pequeños pero no cero\n",
    "   - Lasso: Penalización L1, coeficientes pueden llegar exactamente a cero\n",
    "\n",
    "2. **Capacidad de selección de variables:**\n",
    "   - Lasso puede identificar automáticamente las variables más importantes\n",
    "   - Ridge mantiene todas las variables pero con pesos reducidos\n",
    "\n",
    "3. **Trade-offs:**\n",
    "   - Interpretabilidad vs. Rendimiento predictivo\n",
    "   - Simplicidad del modelo vs. Complejidad\n",
    "\n",
    "### Aplicaciones prácticas:\n",
    "\n",
    "**Usar Lasso cuando:**\n",
    "- Tienes muchas variables y quieres identificar las más importantes\n",
    "- La interpretabilidad es crucial\n",
    "- Quieres un modelo más simple y fácil de explicar\n",
    "\n",
    "**Usar Ridge cuando:**\n",
    "- Todas las variables podrían ser relevantes\n",
    "- El rendimiento predictivo es la prioridad máxima\n",
    "- Quieres evitar la eliminación de variables potencialmente útiles\n",
    "\n",
    "### Próximos pasos:\n",
    "1. Probar con el dataset real de ingresos de Perú\n",
    "2. Experimentar con Elastic Net (combinación de Ridge y Lasso)\n",
    "3. Aplicar estos conceptos a otros problemas de regresión"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
