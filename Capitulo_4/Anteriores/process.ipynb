{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa4f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando el script de preprocesamiento (versión simplificada) ---\n",
      "Filtrado de datos: Se eliminaron 1748 observaciones con ingreso cero.\n",
      "El dataset ahora contiene 6193 observaciones.\n",
      "Se eliminaron 14 columnas constantes.\n",
      "\n",
      "--- ¡Proceso completado! ---\n",
      "Datos listos para modelar (solo ingresos positivos) guardados en: 'predict_income_2020_2.csv'\n"
     ]
    }
   ],
   "source": [
    "# preprocess_data.py (versión simplificada para manual introductorio)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "def preprocess_data(input_path='predict_income_2020.csv', output_path='predict_income_2020_2.csv'):\n",
    "    \"\"\"\n",
    "    Carga los datos crudos, los limpia, preprocesa y guarda un archivo CSV listo para el modelado.\n",
    "    \n",
    "    SIMPLIFICACIÓN METODOLÓGICA:\n",
    "    Este script filtra y se queda ÚNICAMENTE con las observaciones donde el ingreso es mayor a cero.\n",
    "    Esta decisión se toma para simplificar el problema en un contexto introductorio.\n",
    "\n",
    "    Pasos de preprocesamiento:\n",
    "    1. Carga los datos.\n",
    "    2. FILTRA los datos para mantener solo ingmo2hd > 0.\n",
    "    3. Elimina columnas constantes.\n",
    "    4. Transforma 'ingmo2hd' a una escala logarítmica (ahora sin ceros, np.log es suficiente).\n",
    "    5. Aplica StandardScaler a numéricas y OneHotEncoder a categóricas.\n",
    "    6. Guarda el DataFrame procesado en un nuevo archivo CSV.\n",
    "    \"\"\"\n",
    "    print(\"--- Iniciando el script de preprocesamiento (versión simplificada) ---\")\n",
    "\n",
    "    # --- PASO 1: Carga y limpieza inicial ---\n",
    "    data = pd.read_csv(input_path, encoding=\"latin-1\")\n",
    "    if 'Unnamed: 0' in data.columns:\n",
    "        data = data.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    # --- PASO 2: FILTRADO DE DATOS (EL PASO CLAVE) ---\n",
    "    original_rows = len(data)\n",
    "    data_positive_income = data[data['ingmo2hd'] > 0].copy()\n",
    "    filtered_rows = len(data_positive_income)\n",
    "    print(f\"Filtrado de datos: Se eliminaron {original_rows - filtered_rows} observaciones con ingreso cero.\")\n",
    "    print(f\"El dataset ahora contiene {filtered_rows} observaciones.\")\n",
    "    \n",
    "    # --- PASO 3: Eliminar columnas constantes (después del filtrado) ---\n",
    "    cols_to_drop = [col for col in data_positive_income.columns if data_positive_income[col].nunique() == 1]\n",
    "    data_cleaned = data_positive_income.drop(columns=cols_to_drop)\n",
    "    print(f\"Se eliminaron {len(cols_to_drop)} columnas constantes.\")\n",
    "    \n",
    "    # --- PASO 4: Transformar la variable objetivo ---\n",
    "    # Como ya no hay ceros, podemos usar np.log. np.log1p sigue siendo seguro y una buena práctica.\n",
    "    data_cleaned['log_ingmo2hd'] = np.log1p(data_cleaned['ingmo2hd'])\n",
    "    data_cleaned = data_cleaned.drop(columns=['ingmo2hd'])\n",
    "    \n",
    "    # Separar características (X) y objetivo (y)\n",
    "    X = data_cleaned.drop('log_ingmo2hd', axis=1)\n",
    "    y = data_cleaned['log_ingmo2hd']\n",
    "\n",
    "    # --- PASO 5: Preprocesamiento de X ---\n",
    "    numeric_features = ['p208a_20', 'p104_20', 'p301b_20']\n",
    "    categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', drop='if_binary', sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    X_processed_array = preprocessor.fit_transform(X)\n",
    "    new_feature_names = preprocessor.get_feature_names_out()\n",
    "    X_processed_df = pd.DataFrame(X_processed_array, columns=new_feature_names)\n",
    "    \n",
    "    # --- PASO 6: Combinar y Guardar ---\n",
    "    final_df = pd.concat([X_processed_df, y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n--- ¡Proceso completado! ---\\nDatos listos para modelar (solo ingresos positivos) guardados en: '{output_path}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
