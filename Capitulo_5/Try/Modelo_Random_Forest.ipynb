{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce74652",
   "metadata": {},
   "source": [
    "\n",
    "# Modelo Random Forest: Clasificaci√≥n de Nivel de Contagios\n",
    "## Paso 1: Definici√≥n del Problema y Preparaci√≥n Inicial\n",
    "\n",
    "### üéØ Objetivo de la Actividad\n",
    "En este primer cap√≠tulo, sentaremos las bases para construir un modelo de clasificaci√≥n. Aprenderemos a definir correctamente nuestro problema de Machine Learning y a preparar el entorno y los datos para el an√°lisis.\n",
    "\n",
    "### üìã Tareas de este cap√≠tulo:\n",
    "1.  **Cargar las librer√≠as** necesarias para el an√°lisis de datos.\n",
    "2.  **Importar el conjunto de datos** desde un archivo formato Stata (`.dta`).\n",
    "3.  **Realizar una exploraci√≥n inicial** para entender la dimensionalidad y estructura de los datos.\n",
    "4.  **Crear la variable dependiente (objetivo)**, que es la variable que nuestro modelo intentar√° predecir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de213c5",
   "metadata": {},
   "source": [
    "### 1.1. Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d98645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb16b93",
   "metadata": {},
   "source": [
    "### 1.2. Carga del Conjunto de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23043b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nombre_archivo = 'merge_RENAMU_GASTO_V.dta'\n",
    "try:\n",
    "    df = pd.read_stata(nombre_archivo)\n",
    "    print(f\"‚úÖ Base de datos '{nombre_archivo}' cargada exitosamente.\")\n",
    "    print(f\"üìä El conjunto de datos tiene {df.shape[0]} filas y {df.shape[1]} columnas.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: No se pudo encontrar el archivo '{nombre_archivo}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e3c50",
   "metadata": {},
   "source": [
    "### 1.3. Exploraci√≥n Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2868071",
   "metadata": {},
   "source": [
    "### 1.4. Creaci√≥n de la Variable Dependiente (Objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mediana_contagiados = df['contagiados'].median()\n",
    "print(f\"üìä La mediana de la variable 'contagiados' es: {mediana_contagiados}\")\n",
    "\n",
    "df['nivel_contagios'] = df['contagiados'].apply(lambda x: 'ALTO' if x > mediana_contagiados else 'BAJO')\n",
    "print(\"\\n‚úÖ Columna 'nivel_contagios' creada exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9361a5",
   "metadata": {},
   "source": [
    "\n",
    "# Paso 2: Preprocesamiento y Limpieza de Datos\n",
    "\n",
    "### üéØ Objetivo de la Actividad\n",
    "En esta secci√≥n, prepararemos todos los datos para que puedan ser utilizados por el modelo de Machine Learning. Realizaremos una limpieza exhaustiva para manejar valores faltantes, corregir inconsistencias y asegurar que todas las variables est√©n en formato num√©rico.\n",
    "\n",
    "### üìã Tareas de este cap√≠tulo:\n",
    "1.  **Limpieza Global de Datos**: Aplicaremos un conjunto de reglas para limpiar todas las variables del dataset en un solo paso.\n",
    "2.  **Selecci√≥n Final de Variables**: Definiremos los conjuntos `X` (predictoras) y `y` (objetivo) a partir de los datos ya limpios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eed7d4",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. Limpieza y Preparaci√≥n Exhaustiva de Datos\n",
    "\n",
    "A continuaci√≥n, ejecutaremos una celda de c√≥digo que realiza todas las tareas de limpieza necesarias. Aunque el c√≥digo es extenso, las acciones principales son:\n",
    "- **Eliminar columnas no informativas**: Se descartan identificadores √∫nicos, metadatos y variables de texto libre.\n",
    "- **Evitar Fuga de Datos**: Se elimina la variable `contagiados`, ya que se us√≥ para crear nuestro objetivo.\n",
    "- **Corregir datos an√≥malos**: Se ajustan valores il√≥gicos, como montos negativos.\n",
    "- **Manejar valores faltantes**: La mayor√≠a de las variables de encuesta (`P66`, `P67`, etc.) tienen muchos datos faltantes. Asumiremos que un dato faltante significa \"No\" o \"Cero\" y los rellenaremos con `0`.\n",
    "- **Estandarizar la codificaci√≥n**: Nos aseguraremos de que todas las variables de tipo \"S√≠/No\" usen un formato binario consistente (1 para \"S√≠\", 0 para \"No\").\n",
    "- **Convertir todo a num√©rico**: Se asegura que todas las columnas finales sean de tipo num√©rico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc55a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear una copia del DataFrame para no alterar el original.\n",
    "df_clean = df.copy()\n",
    "\n",
    "# --- 1. Eliminar columnas no informativas o problem√°ticas ---\n",
    "# Se eliminan identificadores, metadatos, texto libre y la variable de fuga de datos.\n",
    "cols_to_drop = [\n",
    "    'UBIGEO', 'DEPARTAMENTO', 'PROVINCIA', 'DISTRITO', # Identificadores\n",
    "    'VFI_P66', 'VFI_P67', 'VFI_P68', '_merge',       # Metadatos de la encuesta\n",
    "    'P67_11_O', 'P68_8_O',                           # Columnas de texto \"Otro\"\n",
    "    'contagiados'                                   # ¬°Variable de fuga de datos!\n",
    "]\n",
    "df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "print(f\"‚úÖ Se eliminaron {len(cols_to_drop)} columnas no informativas.\")\n",
    "\n",
    "\n",
    "# --- 2. Corregir y transformar variables espec√≠ficas ---\n",
    "# Corregir valores negativos en MONTO_GIRADO.\n",
    "df_clean.loc[df_clean['MONTO_GIRADO'] < 0, 'MONTO_GIRADO'] = 0\n",
    "\n",
    "# Convertir 'mes' y 'year' de texto a n√∫mero.\n",
    "df_clean[['mes', 'year']] = df_clean[['mes', 'year']].apply(pd.to_numeric, errors='coerce')\n",
    "print(\"‚úÖ Se corrigi√≥ MONTO_GIRADO y se convirtieron 'mes' y 'year' a num√©rico.\")\n",
    "\n",
    "\n",
    "# --- 3. Manejo de Valores Faltantes (Imputaci√≥n) ---\n",
    "# En las variables de encuesta (PXX), los valores faltantes se imputar√°n con 0.\n",
    "# Esto asume que si no hay respuesta, la actividad no se realiz√≥.\n",
    "p_cols = [col for col in df_clean.columns if col.startswith('P')]\n",
    "df_clean[p_cols] = df_clean[p_cols].fillna(0)\n",
    "print(f\"‚úÖ Se imputaron valores faltantes con 0 en {len(p_cols)} columnas de encuesta.\")\n",
    "\n",
    "\n",
    "# --- 4. Estandarizaci√≥n de la Codificaci√≥n Binaria ---\n",
    "# Algunas preguntas usan 2 para \"No\". Las convertimos a 0 para mantener el est√°ndar 0=No, 1=S√≠.\n",
    "p66_recode_2_to_0 = ['P66_1', 'P66_2', 'P66_3', 'P66_4', 'P66_5', 'P66_6', 'P66_7', 'P66_8', 'P66_9', 'P66_10']\n",
    "for col in p66_recode_2_to_0:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].replace(2, 0)\n",
    "\n",
    "# Otras preguntas usan valores > 1 para \"S√≠\". Las convertimos a 1.\n",
    "p_recode_gt0_to_1 = [col for col in df_clean.columns if col.startswith(('P67_', 'P68_'))]\n",
    "for col in p_recode_gt0_to_1:\n",
    "    df_clean[col] = df_clean[col].apply(lambda x: 1 if x > 0 else 0)\n",
    "print(\"‚úÖ Se estandariz√≥ la codificaci√≥n de variables binarias (0=No, 1=S√≠).\")\n",
    "\n",
    "\n",
    "# --- 5. Verificaci√≥n Final ---\n",
    "# Comprobar que todas las columnas son num√©ricas y no hay valores nulos.\n",
    "print(\"\\n--- Verificaci√≥n Final del DataFrame Limpio ---\")\n",
    "df_clean.info()\n",
    "\n",
    "print(\"\\n‚úÖ ¬°El preprocesamiento y la limpieza de datos han finalizado!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a2829",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Selecci√≥n Final de Conjuntos de Datos (X, y)\n",
    "\n",
    "Con el DataFrame `df_clean` completamente procesado, el √∫ltimo paso es separar nuestras variables predictoras (`X`) de nuestra variable objetivo (`y`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517321bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Crear el vector objetivo 'y', codificando \"ALTO\" como 1 y \"BAJO\" como 0.\n",
    "y = df_clean['nivel_contagios'].apply(lambda nivel: 1 if nivel == 'ALTO' else 0)\n",
    "\n",
    "# 2. Crear la matriz de caracter√≠sticas 'X' eliminando la columna objetivo.\n",
    "X = df_clean.drop(columns=['nivel_contagios'])\n",
    "\n",
    "print(\"‚úÖ Conjuntos de datos X e y creados a partir de los datos limpios.\")\n",
    "print(f\"Forma de X (predictoras): {X.shape}\")\n",
    "print(f\"Forma de y (objetivo): {y.shape}\")\n",
    "\n",
    "print(\"\\n--- Primeras 5 filas de X (variables predictoras) ---\")\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29248da",
   "metadata": {},
   "source": [
    "\n",
    "### üéì Resumen del Paso 2\n",
    "\n",
    "**Logros de este cap√≠tulo:**\n",
    "- **Datos Listos para el Modelo**: Hemos procesado todas las variables disponibles, creando un conjunto de datos robusto y limpio.\n",
    "- **Estrategia de Limpieza Aplicada**: Se manejaron valores faltantes, se corrigieron datos y se estandariz√≥ la codificaci√≥n de manera eficiente.\n",
    "- **Conjuntos Finales Definidos**: Tenemos nuestros datos `X` (con 62 variables predictoras) y `y` listos para la siguiente fase: entrenamiento y evaluaci√≥n del modelo.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
